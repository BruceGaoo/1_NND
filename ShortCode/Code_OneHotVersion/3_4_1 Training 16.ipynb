{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Lambda\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2                       # number of information bits\n",
    "N = 16                      # code length\n",
    "train_SNR_Eb = 1            # training-Eb/No\n",
    "\n",
    "epochs = 2**18           # number of learning epochs\n",
    "design = [32, 16, 8]      # each list entry defines the number of nodes in a layer\n",
    "batch_size = 2**k            # size of batches for calculation the gradient\n",
    "optimizer = 'adam'           \n",
    "loss = 'mse'                # or 'binary_crossentropy'\n",
    "\n",
    "train_SNR_Es = train_SNR_Eb + 10*np.log10(2 * k/N)      # training-SNR\n",
    "train_sigma = np.sqrt(1/(10**(train_SNR_Es/10)))    # training-sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_adder(a,b,c):\n",
    "    s = (a ^ b) ^ c\n",
    "    c = (a & b) | (c & (a ^ b))\n",
    "    return s,c\n",
    "\n",
    "def add_bool(a,b):\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError('arrays with different length')\n",
    "    k = len(a)\n",
    "    s = np.zeros(k,dtype=bool)\n",
    "    c = False\n",
    "    for i in reversed(range(0,k)):\n",
    "        s[i], c = full_adder(a[i],b[i],c)    \n",
    "    if c:\n",
    "        warnings.warn(\"Addition overflow!\")\n",
    "    return s\n",
    "\n",
    "def inc_bool(a):\n",
    "    k = len(a)\n",
    "    increment = np.hstack((np.zeros(k-1,dtype=bool), np.ones(1,dtype=bool)))\n",
    "    a = add_bool(a,increment)\n",
    "    return a\n",
    "\n",
    "def polar_design_awgn(N, k):\n",
    "    if N == 16:\n",
    "        idx = [0,  1,  2,  4,  8,  3,  5,  6,  9, 10, 12,  7, 11, 13, 14, 15]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    # select k best channels\n",
    "    idx = idx[N-k:]\n",
    "    \n",
    "    A = np.zeros(N, dtype=bool)\n",
    "    A[idx] = True\n",
    "    return A\n",
    "\n",
    "def polar_transform_iter(u):\n",
    "    N = len(u)\n",
    "    n = 1\n",
    "    x = np.copy(u)\n",
    "    stages = np.log2(N).astype(int)\n",
    "    for s in range(0,stages):\n",
    "        i = 0\n",
    "        while i < N:\n",
    "            for j in range(0,n):\n",
    "                idx = i+j\n",
    "                x[idx] = x[idx] ^ x[idx+n]\n",
    "            i=i+2*n\n",
    "        n=2*n\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all possible information words\n",
    "d_bin = np.zeros((2**k,k),dtype=bool)\n",
    "for i in range(1,2**k):\n",
    "    d_bin[i]= inc_bool(d_bin[i-1])\n",
    "\n",
    "# Create sets of all possible codewords (codebook)\n",
    "A = polar_design_awgn(N, k)\n",
    "x = np.zeros((2**k, N),dtype=bool)\n",
    "u = np.zeros((2**k, N),dtype=bool)\n",
    "u[:,A] = d_bin\n",
    "\n",
    "for i in range(0,2**k):\n",
    "    x[i] = polar_transform_iter(u[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Information bits (One-Hot Version)\n",
    "d_oh = np.eye(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modulateBPSK(x):\n",
    "    return -2*x +1;\n",
    "\n",
    "def addNoise(x, sigma):\n",
    "    w = K.random_normal(K.shape(x), mean=0.0, stddev=sigma)\n",
    "    return x + w\n",
    "\n",
    "def return_output_shape(input_shape):  \n",
    "    return input_shape\n",
    "\n",
    "def compose_model(layers):\n",
    "    model = Sequential()\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "    return model\n",
    "\n",
    "def fe(y_true, y_pred):\n",
    "    tmp = tf.multiply(y_true, y_pred)\n",
    "    return 1 - tf.reduce_sum(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define modulator\n",
    "modulator_layers = [Lambda(modulateBPSK, \n",
    "                          input_shape=(N,), output_shape=return_output_shape, name=\"modulator\")]\n",
    "\n",
    "modulator = compose_model(modulator_layers)\n",
    "modulator.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define noise\n",
    "noise_layers = [Lambda(addNoise, arguments={'sigma':train_sigma}, \n",
    "                       input_shape=(N,), output_shape=return_output_shape, name=\"noise\")]\n",
    "\n",
    "noise = compose_model(noise_layers)\n",
    "noise.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define decoder \n",
    "decoder_layers = [Dense(design[0], activation='relu', input_shape=(N,))]\n",
    "for i in range(1,len(design)):\n",
    "    decoder_layers.append(Dense(design[i], activation='relu'))\n",
    "decoder_layers.append(Dense(2**k, activation='softmax'))\n",
    "\n",
    "decoder = compose_model(decoder_layers)\n",
    "decoder.compile(optimizer=optimizer, loss=loss, metrics=[fe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model_layers = modulator_layers + noise_layers + decoder_layers\n",
    "\n",
    "model = compose_model(model_layers)\n",
    "model.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "modulator (Lambda)           (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "noise (Lambda)               (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 36        \n",
      "=================================================================\n",
      "Total params: 1,244\n",
      "Trainable params: 1,244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x, d_oh, batch_size=batch_size, epochs=epochs, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = 1000\n",
    "num_words = 100000      # multiple of test_batch\n",
    "\n",
    "ebn0_min = 0\n",
    "ebn0_max = 5\n",
    "points = 26\n",
    "ebn0 = np.linspace(ebn0_min, ebn0_max, points)\n",
    "\n",
    "bsum = np.zeros(points, dtype=np.float64)\n",
    "be = np.zeros(points, dtype=np.float64)\n",
    "fsum = np.zeros(points, dtype=np.float64)\n",
    "fe = np.zeros(points, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If your data is in the form of symbolic tensors, you should specify the `steps` argument (instead of the `batch_size` argument, because symbolic tensors are expected to produce batches of input data).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f9fdce6c527f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_test_oh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mfe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mbsum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtest_batch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1114\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m                                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m                                          steps=steps)\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m     def predict(self, x,\n",
      "\u001b[0;32m/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    344\u001b[0m                                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                                     \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                                     steps_name='steps')\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_num_samples\u001b[0;34m(ins, batch_size, steps, steps_name)\u001b[0m\n\u001b[1;32m    562\u001b[0m             raise ValueError(\n\u001b[1;32m    563\u001b[0m                 \u001b[0;34m'If your data is in the form of symbolic tensors, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0;34m'you should specify the `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msteps_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'` argument '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0;34m'(instead of the `batch_size` argument, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                 \u001b[0;34m'because symbolic tensors are expected to produce '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: If your data is in the form of symbolic tensors, you should specify the `steps` argument (instead of the `batch_size` argument, because symbolic tensors are expected to produce batches of input data)."
     ]
    }
   ],
   "source": [
    "for i in range(0,points):\n",
    "    SNR = ebn0[i] + 10*np.log10(2 * k/N)\n",
    "    sigma = np.sqrt(1/(10**(SNR/10)))\n",
    "\n",
    "    for ii in range(0,np.round(num_words/test_batch).astype(int)):\n",
    "        \n",
    "        # Source (One-Hot)\n",
    "        d_test_bin = np.random.randint(0,2,size=(test_batch,k)) \n",
    "        b2d = np.array([2**(k-i-1) for i in range(k)])\n",
    "        d_test_dec = np.dot(d_test_bin, b2d)\n",
    "        d_test_oh = tf.one_hot(d_test_dec, 2**k)\n",
    "\n",
    "        # Encoder\n",
    "        x_test = np.zeros((test_batch, N),dtype=bool)\n",
    "        u_test = np.zeros((test_batch, N),dtype=bool)\n",
    "        u_test[:,A] = d_test_bin\n",
    "\n",
    "        for iii in range(0,test_batch):\n",
    "            x_test[iii] = polar_transform_iter(u_test[iii])\n",
    "\n",
    "        # Modulator (BPSK)\n",
    "        s_test = -2*x_test + 1\n",
    "\n",
    "        # Channel (AWGN)\n",
    "        y_test = s_test + sigma*np.random.standard_normal(s_test.shape)\n",
    "\n",
    "        # Decoder\n",
    "        tmp = decoder.evaluate(y_test, d_test_oh, batch_size=test_batch, verbose=0)[1:]\n",
    "        fe[i] += tmp\n",
    "        bsum[i] += test_batch*k\n",
    "        fsum[i] += test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ber = be/bsum\n",
    "fer = fe/fsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_map = np.loadtxt('map/polar/results_polar_map_{}_{}.txt'.format(N,k), delimiter=', ')\n",
    "sigma_map = result_map[:,0]\n",
    "bsum_map = result_map[:,1]\n",
    "be_map = result_map[:,2]\n",
    "fsum_map = result_map[:,3]\n",
    "fe_map = result_map[:,4]\n",
    "\n",
    "ebn0_map = 10*np.log10(1/(sigma_map**2)) - 10*np.log10(2 * k/N)\n",
    "ber_map = be_map/bsum_map\n",
    "fer_map = fe_map/fsum_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Bit-Error-Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "legend = ['NN FER', 'NN BER', 'MAP FER', 'MAP BER']\n",
    "\n",
    "plt.plot(ebn0, fer, 'c') \n",
    "plt.plot(ebn0, ber, 'c--') \n",
    "plt.plot(ebn0_map, fer_map, 'r')\n",
    "plt.plot(ebn0_map, ber_map, 'r--') \n",
    "\n",
    "plt.legend(legend, loc=3)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('$E_b/N_0$')\n",
    "plt.ylabel('FER / BER')    \n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = pd.Series(history.history['loss'])\n",
    "L.name = 'loss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_NN = pd.DataFrame([ebn0, fer, ber]).T\n",
    "P_NN.columns = ['ebn0', 'fer', 'ber']\n",
    "P_MAP = pd.DataFrame([ebn0_map, fer_map, ber_map]).T\n",
    "P_MAP.columns = ['ebn0_map', 'fer_map', 'ber_map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./result/model_16_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.to_csv('./result/LOSS_16_4.csv', header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_NN.to_csv('./result/P_NN_16_4.csv', header = True, index = False)\n",
    "P_MAP.to_csv('./result/P_MAP_16_4.csv', header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
